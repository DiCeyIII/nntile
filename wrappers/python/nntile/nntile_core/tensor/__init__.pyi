from collections.abc import Buffer
from typing import Literal, Sequence, overload

from .tile import Tile, TileTraits

class TensorTraits:
    def __init__(self, shape: Sequence[int],
                 basetile_shape: Sequence[int]) -> None: ...

    @property
    def basetile_shape(self) -> Sequence[int]: ...
    @property
    def grid(self) -> TileTraits: ...

    def get_tile_shape(self, tile_index: Sequence[int]) -> Sequence[int]: ...
    def get_grid_shape(self) -> Sequence[int]: ...

class Tensor(TensorTraits):
    def __init__(self, traits: TensorTraits, distribution: Sequence[int],
                 last_tag: int): ...

    @property
    def distribution(self) -> Sequence[int]: ...
    @property
    def next_tag(self) -> int: ...

    def from_array(self, array: Buffer) -> None: ...
    def to_array(self, array: Buffer) -> None: ...

    def get_tile(self, linear_offset: int) -> Tile: ...
    def print_scalar_async(self) -> None: ...

    def set_reduction_add(self) -> None: ...
    def set_reduction_hypot(self) -> None: ...
    def set_reduction_maxsumexp(self) -> None: ...

    def invalidate_submit(self) -> None: ...
    def unregister(self) -> None: ...
    def wont_use(self) -> None: ...

def tensor_from_array(tensor: Tensor, array: Buffer): ...
def tensor_to_array(tensor: Tensor, array: Buffer): ...

class Tensor_bf16(Tensor): ...
class Tensor_bool(Tensor): ...
class Tensor_fp32(Tensor): ...
class Tensor_fp32_fast_tf32(Tensor): ...
class Tensor_fp64(Tensor): ...
class Tensor_int64(Tensor): ...

class TransOp:
    def __init__(self, value_: Literal[0, 1]) -> None: ...

def gemm_async_fp32(
    alpha: float, transA: TransOp, A: Tensor_fp32, transB: TransOp,
    B: Tensor_fp32, beta: float, C: Tensor_fp32, ndim: int, batch_ndim: int,
    redux: int) -> None: ...
def gemm_fp32(
    alpha: float, transA: TransOp, A: Tensor_fp32, transB: TransOp,
    B: Tensor_fp32, beta: float, C: Tensor_fp32, ndim: int, batch_ndim: int,
    redux: int) -> None: ...

def relu_async_fp32(A: Tensor_fp32) -> None: ...
def relu_fp32(A: Tensor_fp32) -> None: ...

def relu_forward_async_fp32(src: Tensor_fp32, dst: Tensor_fp32) -> None: ...
def relu_forward_fp32(src: Tensor_fp32, dst: Tensor_fp32) -> None: ...

def silu_forward_async_fp32(src: Tensor_fp32, dst: Tensor_fp32) -> None: ...
def silu_forward_fp32(src: Tensor_fp32, dst: Tensor_fp32) -> None: ...

def relu_backward_async_fp32(x: Tensor_fp32, dy: Tensor_fp32,
                             dx: Tensor_fp32) -> None: ...
def relu_backward_fp32(x: Tensor_fp32, dy: Tensor_fp32,
                       dx: Tensor_fp32) -> None: ...

def silu_backward_async_fp32(x: Tensor_fp32, dy: Tensor_fp32,
                             dx: Tensor_fp32) -> None: ...
def silu_backward_fp32(x: Tensor_fp32, dy: Tensor_fp32,
                       dx: Tensor_fp32) -> None: ...

def drelu_async_fp32(A: Tensor_fp32) -> None: ...
def drelu_fp32(A: Tensor_fp32) -> None: ...

def fill_async_fp32(val: float, A: Tensor_fp32) -> None: ...
def fill_fp32(val: float, A: Tensor_fp32) -> None: ...

def sum_slice_async_fp32(alpha: float, src: Tensor_fp32, beta: float,
                         dst: Tensor_fp32, axis: int, redux: int) -> None: ...
def sum_slice_fp32(alpha: float, src: Tensor_fp32, beta: float,
                   dst: Tensor_fp32, axis: int, redux: int) -> None: ...

def sum_fiber_async_fp32(
    alpha: float, src: Tensor_fp32, beta: float,
    dst: Tensor_fp32, axis: int, batch_ndim: int, redux: int) -> None: ...

def norm_slice_async_fp32(alpha: float, src: Tensor_fp32, beta: float,
                          dst: Tensor_fp32, axis: int, redux: int) -> None: ...

def pow_async_fp32(alpha: float, exp: float, A: Tensor_fp32) -> None: ...

def sumnorm_async_fp32(src: Tensor_fp32,
                       dst: Tensor_fp32, axis: int) -> None: ...

def flash_softmax_gemm_async_fp32(
    Q: Tensor_fp32, K: Tensor_fp32, V: Tensor_fp32, mask: Tensor_bool,
    maxsumexp: Tensor_fp32, dst: Tensor_fp32, tmp: Tensor_fp32,
    redux: int) -> None: ...

def flash_softmax_gemm_backward_async_fp32(
    Q: Tensor_fp32, dQ: Tensor_fp32, K: Tensor_fp32, dK: Tensor_fp32,
    V: Tensor_fp32, dV: Tensor_fp32, mask: Tensor_bool,
    maxsumexp: Tensor_fp32, dst_grad: Tensor_fp32, tmp: Tensor_fp32,
    tmp_grad: Tensor_fp32, tmp_sumprod_slice: Tensor_fp32,
    redux: int) -> None: ...

def softmax_async_fp32(maxsumexp: Tensor_fp32, src: Tensor_fp32,
                  alpha: float, dst: Tensor_fp32, axis: int) -> None: ...

def softmax_inplace_async_fp32(maxsumexp: Tensor_fp32, alpha: float,
                               dst: Tensor_fp32, axis: int) -> None: ...

def scatter_async_fp32(src: Tensor_fp32, dst: Tensor_fp32) -> None: ...

def randn_async_fp32(
    dst: Tensor_fp32, start: Sequence[int], underlying_shape: Sequence[int],
    seed: int, mean: float, stddev: float) -> None: ...

def prod_async_fp32(src: Tensor_fp32, dst: Tensor_fp32) -> None: ...

def nrm2_async_fp32(alpha: float, src: Tensor_fp32, beta: float,
                    dst: Tensor_fp32, tmp: Tensor_fp32) -> None: ...

# ruff: noqa: E741
def normalize_async_fp32(
    gamma_beta: Tensor_fp32, src: Tensor_fp32, dst: Tensor_fp32, l: int,
    eps: float, axis: int) -> None: ...

def flash_maxsumexp_async_fp32(
    Q: Tensor_fp32, K: Tensor_fp32, mask: Tensor_bool, maxsumexp: Tensor_fp32,
    tmp: Tensor_fp32, redux: int) -> None: ...

def maxsumexp_async_fp32(src: Tensor_fp32, dst: Tensor_fp32, axis: int,
                         redux: int) -> None: ...

def add_slice_async_fp32(alpha: float, src: Tensor_fp32, beta: float,
                         dst: Tensor_fp32, axis: int) -> None: ...

def add_slice3_async_fp32(
    alpha: float, src1: Tensor_fp32, beta: float, src2: Tensor_fp32,
    dst: Tensor_fp32, axis: int) -> None: ...

def add_async_fp32(
    alpha: float, src: Tensor_fp32, beta: float, dst: Tensor_fp32) -> None: ...

def add_scalar_async_fp32(
    alpha: float, beta: float, dst: Tensor_fp32) -> None: ...

def add_fiber_async_fp32(
    alpha: float, src: Tensor_fp32, beta: float, dst: Tensor_fp32,
    axis: int, batch_ndim: int) -> None: ...

def prod_slice_async_fp32(
    src: Tensor_fp32, alpha: int, dst: Tensor_fp32, axis: int) -> None: ...

def prod_fiber_async_fp32(
    src: Tensor_fp32, alpha: float, dst: Tensor_fp32, axis: int) -> None: ...

def prod_fiber3_async_fp32(src1: Tensor_fp32, alpha: float, src2: Tensor_fp32,
                           dst: Tensor_fp32, axis: int) -> None: ...

def gather_async_fp32(src: Tensor_fp32, dst: Tensor_fp32) -> None: ...

def copy_intersection_async_fp32(
    src: Tensor_fp32, src_offset: Sequence[int],
    dst: Tensor_fp32, dst_offset: Sequence[int]) -> None: ...

def copy_async_fp32(src: Tensor_fp32, dst: Tensor_fp32) -> None: ...

def clear_async_fp32(dst: Tensor_fp32) -> None: ...

@overload
def axpy_async_fp32(
    alpha: float, src: Tensor_fp32, dst: Tensor_fp32) -> None: ...

@overload
def axpy_async_fp32(
    alpha: Tensor_fp32, src: Tensor_fp32, dst: Tensor_fp32) -> None: ...

def sqrt_async_fp32(src: Tensor_fp32, dst: Tensor_fp32) -> None: ...

def sqrt_inplace_async_fp32(A: Tensor_fp32) -> None: ...

def maximum_async_fp32(src: Tensor_fp32, dst: Tensor_fp32) -> None: ...

def addcdiv_async_fp32(val: float, eps: float, nom: Tensor_fp32,
                       denom: Tensor_fp32, src: Tensor_fp32) -> None: ...

def logsumexp_async_fp32(src: Tensor_fp32, dst: Tensor_fp32) -> None: ...

def total_sum_accum_async_fp32(
    alpha: float, logsumexp: Tensor_fp32, src: Tensor_fp32,
    labels: Tensor_int64, val: Tensor_fp32) -> None: ...

def subtract_indexed_outputs_async_fp32(
    val: float, labels: Tensor_int64, dst: Tensor_fp32) -> None: ...

def scal_async_fp32(
    alpha: float, src: Tensor_fp32, dst: Tensor_fp32) -> None: ...

def adam_step_async_fp32(
    num_iter: int, beta_1: float, beta_2: float, eps: float, lr: float,
    weight_decay: float, grad: Tensor_fp32, first_moment: Tensor_fp32,
    second_moment: Tensor_fp32, p: Tensor_fp32) -> None: ...

def adamw_step_async_fp32(
    num_iter: int, beta_1: float, beta_2: float, eps: float, lr: float,
    weight_decay: float, grad: Tensor_fp32, first_moment: Tensor_fp32,
    second_moment: Tensor_fp32, p: Tensor_fp32) -> None: ...

def scal_inplace_async_fp32(alpha: float, data: Tensor_fp32) -> None: ...

def sumprod_slice_async_fp32(
    alpha: float, src1: Tensor_fp32, src2: Tensor_fp32, beta: float,
    dst: Tensor_fp32, axis: int, redux: int) -> None: ...

def sumprod_fiber_async_fp32(
    alpha: float, src1: Tensor_fp32, src2: Tensor_fp32, beta: float,
    dst: Tensor_fp32, axis: int, redux: int) -> None: ...

def gelu_async_fp32(A: Tensor_fp32) -> None: ...

def gelu_backward_async_fp32(x: Tensor_fp32, dy: Tensor_fp32, dx) -> None: ...

def gelutanh_async_fp32(src: Tensor_fp32, dst: Tensor_fp32) -> None: ...

def gelutanh_inplace_async_fp32(A: Tensor_fp32) -> None: ...

def gelutanh_backward_async_fp32(x: Tensor_fp32, dy: Tensor_fp32,
                                 dx: Tensor_fp32) -> None: ...

def dgelu_async_fp32(A: Tensor_fp32) -> None: ...

def dgelutanh_async_fp32(A: Tensor_fp32) -> None: ...

def embedding_async_fp32(index: Tensor_int64, vocab: Tensor_fp32,
                         embed: Tensor_fp32, axis: int) -> None: ...

def embedding_backward_async_fp32(
    index: Tensor_int64, embed: Tensor_fp32, vocab: Tensor_fp32,
    axis: int, redux: int) -> None: ...

def mask_scalar_async_fp32(mask: Tensor_bool, val: float, A: Tensor_fp32,
                           batch_ndim: int) -> None: ...

def hypot_async_fp32(alpha: float, src: Tensor_fp32,
                     beta: float, dst: Tensor_fp32) -> None: ...

def hypot_scalar_inverse_async_fp32(
    eps: float, alpha: float, dst: Tensor_fp32) -> None: ...

def transpose_async_fp32(alpha: float, src: Tensor_fp32, dst: Tensor_fp32,
                         ndim: int) -> None: ...
