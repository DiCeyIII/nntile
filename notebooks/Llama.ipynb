{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a7c9e3f-1c2f-44e3-a887-025863461efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminary setup of execution environment\n",
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "nntile_dir = Path.cwd() / \"..\"\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Limit CUDA visibility\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\" # Disable BLAS parallelism\n",
    "os.environ[\"PYTHONPATH\"] = str(nntile_dir / \"build-1.4.7\" / \"wrappers\" / \"python\") # Path to a binary dir of NNTile Python wrappers\n",
    "\n",
    "# All StarPU environment variables are available at https://files.inria.fr/starpu/doc/html/ExecutionConfigurationThroughEnvironmentVariables.html\n",
    "os.environ[\"STARPU_NCPU\"] = \"1\" # Use only 1 CPU core\n",
    "os.environ[\"STARPU_NCUDA\"] = \"1\" # Use only 1 CUDA device\n",
    "os.environ[\"STARPU_SILENT\"] = \"1\" # Do not show lots of StarPU outputs\n",
    "os.environ[\"STARPU_SCHED\"] = \"dmdasd\" # Name StarPU scheduler to be used\n",
    "os.environ[\"STARPU_FXT_TRACE\"] = \"0\" # Do not generate FXT traces\n",
    "os.environ[\"STARPU_FXT_EVENTS\"] = \"TASK\" # Which traces to generate\n",
    "os.environ[\"STARPU_WORKERS_NOBIND\"] = \"1\" # Do not bind workers (it helps if several instances of StarPU run in parallel)\n",
    "os.environ[\"STARPU_PROFILING\"] = \"1\" # This enables logging performance of workers and bandwidth of memory nodes\n",
    "os.environ[\"STARPU_HOME\"] = str(Path.cwd() / \"starpu\") # Main directory in which StarPU stores its configuration files\n",
    "os.environ[\"STARPU_PERF_MODEL_DIR\"] = str(Path(os.environ[\"STARPU_HOME\"]) / \"sampling\") # Main directory in which StarPU stores its performance model files\n",
    "os.environ[\"STARPU_PERF_MODEL_HOMOGENEOUS_CPU\"] = \"1\" # Assume all CPU cores are equal\n",
    "os.environ[\"STARPU_PERF_MODEL_HOMOGENEOUS_CUDA\"] = \"1\" # Assume all CUDA devices are equal\n",
    "os.environ[\"STARPU_HOSTNAME\"] = \"GPT2_example\" # Force the hostname to be used when managing performance model files\n",
    "os.environ[\"STARPU_FXT_PREFIX\"] = str(Path(os.environ[\"STARPU_HOME\"]) / \"fxt\") # Directory to store FXT traces if enabled\n",
    "\n",
    "# NNTile-related\n",
    "os.environ[\"NNTILE_LOGGER\"] = \"1\" # Enable logger\n",
    "os.environ[\"NNTILE_LOGGER_SERVER_ADDR\"] = \"127.0.0.1\" # Logger will be running on the localhost\n",
    "os.environ[\"NNTILE_LOGGER_SERVER_PORT\"] = \"5001\" # Port for logger server\n",
    "os.environ[\"NNTILE_LOGGER_CLIENT_PORT\"] = \"6006\" # Port for client web interface of the logger\n",
    "os.environ[\"NNTILE_LOGGER_SERVER_DIR\"] = str(Path.cwd() / \"logs\") # Directory to store logs on the logger server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02eec0e3-bd27-4caa-8081-202cd423edc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 23:52:51.906689: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-07 23:52:51.945200: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Launch logger if needed\n",
    "if os.getenv(\"NNTILE_LOGGER\", \"0\") != \"0\":\n",
    "    logger_env = os.environ.copy()\n",
    "    logger_env.update({\n",
    "        \"LOG_DIR\": os.getenv(\"NNTILE_LOGGER_SERVER_DIR\"),\n",
    "        \"SPLIT_HOURS\": \"720\",\n",
    "        \"CLEAR_LOGS\": \"0\",\n",
    "        \"SERVER_PORT\": os.getenv(\"NNTILE_LOGGER_SERVER_PORT\")\n",
    "    })\n",
    "    logger_proc = subprocess.Popen([\"python\", nntile_dir / \"logger\" / \"server.py\"], env=logger_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b152f53e-78c7-4900-b8d1-72352189b448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.mlspace/envs/mikhalev/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Prepare TinyStories dataset into train.bin file\n",
    "!python ../wrappers/python/examples/causal_lm_data_preparation.py --seq-len=1024 --batch-size=1024 --dataset-select=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86bd4e10-4626-4140-a989-aced3dfac91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CondaError: Run 'conda init' before 'conda activate'\n",
      "\n",
      "Namespace(remote_model_name='kimihailv/llama-1.3b', pretrained='local', checkpoint_path='', config_path='../wrappers/python/examples/llama_1.3b_config.json', save_checkpoint_path='.model', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=128, minibatch_size=8, minibatch_size_tile=4, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='bf16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=5, logger=True, logger_server_addr='127.0.0.1', logger_server_port=5001)\n",
      "LlamaConfig {\n",
      "  \"activation_function\": \"silu\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaCasualForLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"fp32\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"flashattention\": false,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5504,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"redux\": false,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.39.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32002\n",
      "}\n",
      "\n",
      "Trying to connect to 127.0.0.1:5001\n",
      "WORKER COUNT: 2\n",
      "BUS COUNT: 2\n",
      "MEMNODES COUNT: 2\n",
      "IS initialized : 1\n",
      "StarPU + NNTile + MPI init in 0.3445148468017578 seconds\n",
      "LlamaConfigNNTile(vocab_size=32002, vocab_embed_dim_tile=2048, hidden_size=2048, hidden_size_tile=2048, max_position_embeddings=4096, intermediate_size=5504, intermediate_size_tile=5504, n_attention_head=16, n_head_tile=16, num_key_value_heads=16, activation_function='silu', redux=False, dtype='bf16', eos_token_id=2, bos_token_id=1, attention_bias=False, attention_dropout=0.0, rope_theta=10000.0, rms_norm_eps=1e-05, num_hidden_layers=24, mlp_bias=False, flash_attention=False)\n",
      "Converting PyTorch model to NNTile requires 12.765560865402222 seconds\n",
      "From PyTorch loader to NNTile batches in 0.03267550468444824 seconds\n",
      "Params+grads (GB): 5.012\n",
      "Activations  (GB): 26.227\n",
      "Optimizer    (GB): 5.012\n",
      "Persistent   (GB): 36.251\n",
      "Temporaries  (GB): 33.152\n",
      "Batch=1/8 Epoch=1/5 Loss=10.838861465454102\n",
      "Batch=2/8 Epoch=1/5 Loss=9.13225269317627\n",
      "Batch=3/8 Epoch=1/5 Loss=9.903736114501953\n",
      "Batch=4/8 Epoch=1/5 Loss=10.002500534057617\n",
      "Batch=5/8 Epoch=1/5 Loss=8.95867919921875\n",
      "Batch=6/8 Epoch=1/5 Loss=8.215764045715332\n",
      "Batch=7/8 Epoch=1/5 Loss=7.935334205627441\n",
      "Batch=8/8 Epoch=1/5 Loss=7.63472843170166\n",
      "Batch=1/8 Epoch=2/5 Loss=7.418085098266602\n",
      "Batch=2/8 Epoch=2/5 Loss=7.092955589294434\n",
      "Batch=3/8 Epoch=2/5 Loss=6.877058506011963\n",
      "Batch=4/8 Epoch=2/5 Loss=6.812618255615234\n",
      "Batch=5/8 Epoch=2/5 Loss=6.66770076751709\n",
      "Batch=6/8 Epoch=2/5 Loss=6.580122947692871\n",
      "Batch=7/8 Epoch=2/5 Loss=6.504814147949219\n",
      "Batch=8/8 Epoch=2/5 Loss=6.402790546417236\n",
      "Batch=1/8 Epoch=3/5 Loss=6.337490558624268\n",
      "Batch=2/8 Epoch=3/5 Loss=6.210196018218994\n",
      "Batch=3/8 Epoch=3/5 Loss=6.1953444480896\n",
      "Batch=4/8 Epoch=3/5 Loss=6.213517189025879\n",
      "Batch=5/8 Epoch=3/5 Loss=6.089608669281006\n",
      "Batch=6/8 Epoch=3/5 Loss=6.033819198608398\n",
      "Batch=7/8 Epoch=3/5 Loss=6.078170299530029\n",
      "Batch=8/8 Epoch=3/5 Loss=6.023959159851074\n",
      "Batch=1/8 Epoch=4/5 Loss=6.018298149108887\n",
      "Batch=2/8 Epoch=4/5 Loss=5.873406410217285\n",
      "Batch=3/8 Epoch=4/5 Loss=5.906384468078613\n",
      "Batch=4/8 Epoch=4/5 Loss=5.9104413986206055\n",
      "Batch=5/8 Epoch=4/5 Loss=5.820684432983398\n",
      "Batch=6/8 Epoch=4/5 Loss=5.814004421234131\n",
      "Batch=7/8 Epoch=4/5 Loss=5.8579487800598145\n",
      "Batch=8/8 Epoch=4/5 Loss=5.809586524963379\n",
      "Batch=1/8 Epoch=5/5 Loss=5.812868595123291\n",
      "Batch=2/8 Epoch=5/5 Loss=5.661558628082275\n",
      "Batch=3/8 Epoch=5/5 Loss=5.681150436401367\n",
      "Batch=4/8 Epoch=5/5 Loss=5.69927978515625\n",
      "Batch=5/8 Epoch=5/5 Loss=5.570199966430664\n",
      "Batch=6/8 Epoch=5/5 Loss=5.563338756561279\n",
      "Batch=7/8 Epoch=5/5 Loss=5.612669944763184\n",
      "Batch=8/8 Epoch=5/5 Loss=5.507299900054932\n",
      "NNTile training time: 389.34090757369995 seconds\n",
      "NNTile training throughput tokens/sec: 13466.039396354861\n",
      "NNTile performance (model flops): 111.53552001162846 Tflops/s\n",
      "NNTile loss on the last batch: 5.507299900054932\n",
      "LOGGER SHUTDOWN\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to finetune a pretrained LLaMa model on TinyStories\n",
    "# If logger server is launched, then TensorBoard results can be accessed at localhost:6006\n",
    "!. ~/mikhalev/load-starpu-1.4.7.sh && python ../wrappers/python/examples/llama_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"../wrappers/python/examples/llama_1.3b_config.json\" \\\n",
    "    --optimizer=\"adam\" --lr=1e-4 --dtype=bf16 --nepochs=5 --batch-size=128 --minibatch-size=8 --minibatch-size-tile=4 \\\n",
    "    --dataset-file=\"tinystories/train.bin\" --logger --logger-server-addr=127.0.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79af2a7b-0538-4956-a776-25febb4447f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CondaError: Run 'conda init' before 'conda activate'\n",
      "\n",
      "Namespace(remote_model_name='kimihailv/llama-1.3b', pretrained='local', checkpoint_path='', config_path='llama-8b.json', save_checkpoint_path='.model', optimizer='adamw', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=64, minibatch_size=2, minibatch_size_tile=2, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='bf16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=5, logger=True, logger_server_addr='127.0.0.1', logger_server_port=5001)\n",
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"models/llama-3-8b\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.39.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "Trying to connect to 127.0.0.1:5001\n",
      "WORKER COUNT: 2\n",
      "BUS COUNT: 2\n",
      "MEMNODES COUNT: 2\n",
      "StarPU + NNTile + MPI init in 0.40987372398376465 seconds\n",
      "IS initialized : 1\n",
      "LlamaConfigNNTile(vocab_size=128256, vocab_embed_dim_tile=4096, hidden_size=4096, hidden_size_tile=4096, max_position_embeddings=8192, intermediate_size=14336, intermediate_size_tile=14336, n_attention_head=32, n_head_tile=32, num_key_value_heads=8, activation_function='silu', redux=False, dtype='bf16', eos_token_id=2, bos_token_id=1, attention_bias=False, attention_dropout=0.0, rope_theta=10000.0, rms_norm_eps=1e-05, num_hidden_layers=32, mlp_bias=False, flash_attention=False)\n",
      "Converting PyTorch model to NNTile requires 141.97899675369263 seconds\n",
      "From PyTorch loader to NNTile batches in 0.045607566833496094 seconds\n",
      "Params+grads (GB): 29.915\n",
      "Activations  (GB): 21.041\n",
      "Optimizer    (GB): 29.915\n",
      "Persistent   (GB): 80.871\n",
      "Temporaries  (GB): 18.340\n",
      "Segmentation fault (core dumped)\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to finetune a pretrained LLaMa model on TinyStories\n",
    "# If logger server is launched, then TensorBoard results can be accessed at localhost:6006\n",
    "!. ~/mikhalev/load-starpu-1.4.7.sh && python ../wrappers/python/examples/llama_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"llama-8b.json\" \\\n",
    "    --optimizer=\"adamw\" --lr=1e-4 --dtype=bf16 --nepochs=5 --batch-size=64 --minibatch-size=2 --minibatch-size-tile=2 \\\n",
    "    --dataset-file=\"tinystories/train.bin\" --logger --logger-server-addr=127.0.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b29aa48-fb90-4aea-af42-8867b238305b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CondaError: Run 'conda init' before 'conda activate'\n",
      "\n",
      "Namespace(remote_model_name='kimihailv/llama-1.3b', pretrained='local', checkpoint_path='', config_path='llama-8b-4h.json', save_checkpoint_path='.model', optimizer='adamw', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=64, minibatch_size=1, minibatch_size_tile=1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='bf16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='localhost', logger_server_port=5001)\n",
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"models/llama-3-8b\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.39.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 0.44031572341918945 seconds\n",
      "LlamaConfigNNTile(vocab_size=128256, vocab_embed_dim_tile=4096, hidden_size=4096, hidden_size_tile=4096, max_position_embeddings=8192, intermediate_size=14336, intermediate_size_tile=14336, n_attention_head=4, n_head_tile=4, num_key_value_heads=1, activation_function='silu', redux=False, dtype='bf16', eos_token_id=2, bos_token_id=1, attention_bias=False, attention_dropout=0.0, rope_theta=10000.0, rms_norm_eps=1e-05, num_hidden_layers=32, mlp_bias=False, flash_attention=False)\n",
      "Converting PyTorch model to NNTile requires 144.98725199699402 seconds\n",
      "From PyTorch loader to NNTile batches in 0.01029062271118164 seconds\n",
      "Params+grads (GB): 29.915\n",
      "Activations  (GB): 10.521\n",
      "Optimizer    (GB): 29.915\n",
      "Persistent   (GB): 70.351\n",
      "Temporaries  (GB): 5.735\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "Batch=1/1 Epoch=1/1 Loss=12.57446575164795\n",
      "NNTile training time: 27.17301893234253 seconds\n",
      "NNTile training throughput tokens/sec: 2411.804156291083\n",
      "NNTile performance (model flops): 112.48308087791528 Tflops/s\n",
      "NNTile loss on the last batch: 12.57446575164795\n"
     ]
    }
   ],
   "source": [
    "# Launch an external python process to finetune a pretrained LLaMa model on TinyStories\n",
    "# If logger server is launched, then TensorBoard results can be accessed at localhost:6006\n",
    "!. ~/mikhalev/load-starpu-1.4.7.sh && python ../wrappers/python/examples/llama_training.py \\\n",
    "    --restrict=\"cuda\" --pretrained=local --config-path=\"llama-8b-4h.json\" \\\n",
    "    --optimizer=\"adamw\" --lr=1e-4 --dtype=bf16 --nepochs=1 --batch-size=64 --minibatch-size=1 --minibatch-size-tile=1 \\\n",
    "    --dataset-file=\"tinystories/train.bin\" --logger --logger-server-addr=127.0.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "856efbf1-30fe-4f66-9dfc-e9be446ed277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CondaError: Run 'conda init' before 'conda activate'\n",
      "\n",
      "Namespace(remote_model_name='kimihailv/llama-1.3b', pretrained='local', checkpoint_path='', config_path='llama-8b.json', save_checkpoint_path='.model', optimizer='adam', model_path='.model', seq_len=1024, seq_len_tile=-1, batch_size=1024, minibatch_size=1, minibatch_size_tile=1, hidden_size_tile=-1, intermediate_size_tile=-1, n_head_tile=-1, dtype='bf16', restrict='cuda', flash_attention=False, use_redux=False, dataset_path='.data', dataset_file='tinystories/train.bin', lr=0.0001, nepochs=1, logger=False, logger_server_addr='127.0.0.1', logger_server_port=5001)\n",
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"models/llama-3-8b\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.39.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "StarPU + NNTile + MPI init in 0.4578981399536133 seconds\n",
      "LlamaConfigNNTile(vocab_size=128256, vocab_embed_dim_tile=4096, hidden_size=4096, hidden_size_tile=4096, max_position_embeddings=8192, intermediate_size=14336, intermediate_size_tile=14336, n_attention_head=32, n_head_tile=32, num_key_value_heads=8, activation_function='silu', redux=False, dtype='bf16', eos_token_id=2, bos_token_id=1, attention_bias=False, attention_dropout=0.0, rope_theta=10000.0, rms_norm_eps=1e-05, num_hidden_layers=32, mlp_bias=False, flash_attention=False)\n",
      "Converting PyTorch model to NNTile requires 155.3045516014099 seconds\n",
      "From PyTorch loader to NNTile batches in 0.12914347648620605 seconds\n",
      "Params+grads (GB): 29.915\n",
      "Activations  (GB): 10.521\n",
      "Optimizer    (GB): 29.915\n",
      "Persistent   (GB): 70.351\n",
      "Temporaries  (GB): 9.186\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "[starpu][_starpu_fxt_flush_callback] FxT is flushing trace to disk ! This can impact performance.\n",
      "[starpu][_starpu_fxt_flush_callback] Maybe you should increase the value of STARPU_TRACE_BUFFER_SIZE ?\n",
      "Batch=1/1 Epoch=1/1 Loss=12.568790435791016\n",
      "NNTile training time: 369.7214424610138 seconds\n",
      "NNTile training throughput tokens/sec: 2836.1243887296846\n",
      "NNTile performance (model flops): 132.27276690985468 Tflops/s\n",
      "NNTile loss on the last batch: 12.568790435791016\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccbee55-12bf-48e5-b3d3-914f05be14d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-mikhalev]",
   "language": "python",
   "name": "conda-env-.mlspace-mikhalev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
